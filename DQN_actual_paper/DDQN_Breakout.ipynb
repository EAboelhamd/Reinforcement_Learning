{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-15 22:29:57,354] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")\n",
    "\n",
    "# Considered Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) \n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari iamges. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            sess: Tensorflow session object\n",
    "            state: Image [210, 160, 3] input dimension and rgb channels.\n",
    "\n",
    "        Returns:\n",
    "            Processed [84, 84, 1] grayscale values representing state.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both Q-Network(action selection) and Target Network(policy evalutaion).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Tensorboard summary write\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Building the graph\n",
    "            self._build_model()\n",
    "            # Writing summaries to directory\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Input: 4 frames of shape 84 * 84 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Id which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # first convolutional layer- input [84 * 84 * 4], filter [8 * 8], num_filters 32, stride 4\n",
    "        conv1 = tf.contrib.layers.conv2d(X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        # second convolutional layer- input [20 * 20 * 32], filter [4 * 4], num_filters 64, stride 2\n",
    "        conv2 = tf.contrib.layers.conv2d(conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        # third convolutional layer- input [9 * 9 * 64], filter [3 * 3], num_filters 64, stride 1\n",
    "        conv3 = tf.contrib.layers.conv2d(conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # first fully connected layer- input [7 * 7 * 64], num_filters 512\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        # seconf fully connexted layer- input 512, num_filters 4, linear\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # getting the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # calcualting the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.losses = tf.clip_by_value(self.losses, -1, 1)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Passing Optimizer parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Tensorboard summaries\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def predict(self, sess, state):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Arguments:\n",
    "          sess: Tensorflow session\n",
    "          state: State input of shape [batch_size, 4, 84, 84, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: state })\n",
    "\n",
    "    def update(self, sess, state, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Argumentss:\n",
    "          sess: Tensorflow session object\n",
    "          state: Image of shape [batch_size, 4, 84, 84, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: state, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        \n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Arguments:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Arguments:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        \n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Arguments:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI gym environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sample when initializing the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the target estimator every N steps\n",
    "        discount_factor: Lambda time discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action. Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # keeping track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # load a previous checkpoint if avalilable\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # our game policy\n",
    "    policy = make_epsilon_greedy_policy(q_estimator, len(VALID_ACTIONS))\n",
    "\n",
    "    # initializing replay memory with initial experiences\n",
    "    print(\"Initializing replay memory.\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    \n",
    "    for i in range(replay_memory_init_size):\n",
    "        \n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    # recording game video\n",
    "    env.monitor.start(monitor_path,\n",
    "                      resume=True,\n",
    "                      video_callable=lambda count: count % record_video_every == 0)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # saving the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # resetting the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # adding epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # updating the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # printing useful information for debugging\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # taking a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # saving latest transition to replay memory\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # updating statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # sampling a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # calculating q values and targets (Double DQN)\n",
    "            q_values_next = q_estimator.predict(sess, next_states_batch)\n",
    "            best_actions = np.argmax(q_values_next, axis=1)\n",
    "            q_values_next_target = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * \\\n",
    "                discount_factor * q_values_next_target[np.arange(batch_size), best_actions]\n",
    "\n",
    "            # performing gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            # shows an episode has terminated when set to true\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # adding summaries to Tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    env.monitor.close()\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leena/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing replay memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-15 22:32:16,401] Starting new video recorder writing to /Users/leena/Documents/AI/reinforcement-learning/DQN/experiments-100/Breakout-v0/monitor/openaigym.video.0.4011.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copied model parameters to target network.\n",
      "Step 250 (250) @ Episode 1/100, loss: 0.06256594508886337\n",
      "Episode Reward: 2.0\n",
      "Step 360 (610) @ Episode 2/100, loss: 0.0003187861875630915\n",
      "Episode Reward: 3.0\n",
      "Step 177 (787) @ Episode 3/100, loss: 0.000272589735686779\n",
      "Episode Reward: 0.0\n",
      "Step 172 (959) @ Episode 4/100, loss: 0.0009831669740378857\n",
      "Episode Reward: 0.0\n",
      "Step 40 (999) @ Episode 5/100, loss: 0.0006929963710717857\n",
      "Copied model parameters to target network.\n",
      "Step 313 (1272) @ Episode 5/100, loss: 0.00022475063451565802\n",
      "Episode Reward: 3.0\n",
      "Step 261 (1533) @ Episode 6/100, loss: 1.7335725715383887e-05\n",
      "Episode Reward: 2.0\n",
      "Step 181 (1714) @ Episode 7/100, loss: 6.129081884864718e-05\n",
      "Episode Reward: 0.0\n",
      "Step 172 (1886) @ Episode 8/100, loss: 9.596770541975275e-05\n",
      "Episode Reward: 0.0\n",
      "Step 113 (1999) @ Episode 9/100, loss: 0.00011316889140289277\n",
      "Copied model parameters to target network.\n",
      "Step 307 (2193) @ Episode 9/100, loss: 2.6690962840802968e-05\n",
      "Episode Reward: 2.0\n",
      "Step 329 (2522) @ Episode 10/100, loss: 0.00013201184628996998\n",
      "Episode Reward: 3.0\n",
      "Step 318 (2840) @ Episode 11/100, loss: 0.03109264187514782\n",
      "Episode Reward: 2.0\n",
      "Step 159 (2999) @ Episode 12/100, loss: 1.7307083908235654e-05\n",
      "Copied model parameters to target network.\n",
      "Step 235 (3075) @ Episode 12/100, loss: 2.5219726012437604e-05\n",
      "Episode Reward: 1.0\n",
      "Step 355 (3430) @ Episode 13/100, loss: 4.792413892573677e-05\n",
      "Episode Reward: 3.0\n",
      "Step 343 (3773) @ Episode 14/100, loss: 1.2593864084919915e-05\n",
      "Episode Reward: 3.0\n",
      "Step 226 (3999) @ Episode 15/100, loss: 0.09222643822431564\n",
      "Copied model parameters to target network.\n",
      "Step 274 (4047) @ Episode 15/100, loss: 4.8626541683916e-05\n",
      "Episode Reward: 2.0\n",
      "Step 278 (4325) @ Episode 16/100, loss: 7.415079016936943e-05\n",
      "Episode Reward: 2.0\n",
      "Step 176 (4501) @ Episode 17/100, loss: 0.030397985130548477\n",
      "Episode Reward: 0.0\n",
      "Step 267 (4768) @ Episode 18/100, loss: 8.53909514262341e-05\n",
      "Episode Reward: 2.0\n",
      "Step 231 (4999) @ Episode 19/100, loss: 8.1974285421893e-05\n",
      "Copied model parameters to target network.\n",
      "Step 329 (5097) @ Episode 19/100, loss: 0.0001936134067364037\n",
      "Episode Reward: 3.0\n",
      "Step 263 (5360) @ Episode 20/100, loss: 7.264335727086291e-05\n",
      "Episode Reward: 2.0\n",
      "Step 502 (5862) @ Episode 21/100, loss: 0.030480466783046722\n",
      "Episode Reward: 6.0\n",
      "Step 137 (5999) @ Episode 22/100, loss: 6.226333789527416e-05\n",
      "Copied model parameters to target network.\n",
      "Step 368 (6230) @ Episode 22/100, loss: 3.017106791958213e-05\n",
      "Episode Reward: 3.0\n",
      "Step 191 (6421) @ Episode 23/100, loss: 5.365055039874278e-05\n",
      "Episode Reward: 0.0\n",
      "Step 193 (6614) @ Episode 24/100, loss: 7.529632421210408e-05\n",
      "Episode Reward: 0.0\n",
      "Step 259 (6873) @ Episode 25/100, loss: 7.888165418989956e-05\n",
      "Episode Reward: 1.0\n",
      "Step 126 (6999) @ Episode 26/100, loss: 9.194877929985523e-05\n",
      "Copied model parameters to target network.\n",
      "Step 265 (7138) @ Episode 26/100, loss: 7.027501123957336e-05\n",
      "Episode Reward: 2.0\n",
      "Step 169 (7307) @ Episode 27/100, loss: 1.3300209502631333e-05\n",
      "Episode Reward: 0.0\n",
      "Step 249 (7556) @ Episode 28/100, loss: 0.030673805624246597\n",
      "Episode Reward: 2.0\n",
      "Step 343 (7899) @ Episode 29/100, loss: 9.90629632724449e-06\n",
      "Episode Reward: 4.0\n",
      "Step 100 (7999) @ Episode 30/100, loss: 1.0125842891284265e-05\n",
      "Copied model parameters to target network.\n",
      "Step 157 (8056) @ Episode 30/100, loss: 0.00010654048674041405\n",
      "Episode Reward: 0.0\n",
      "Step 266 (8322) @ Episode 31/100, loss: 9.101247269427404e-05\n",
      "Episode Reward: 2.0\n",
      "Step 172 (8494) @ Episode 32/100, loss: 8.798173803370446e-05\n",
      "Episode Reward: 0.0\n",
      "Step 175 (8669) @ Episode 33/100, loss: 9.053174289874732e-05\n",
      "Episode Reward: 0.0\n",
      "Step 224 (8893) @ Episode 34/100, loss: 1.106905983760953e-05\n",
      "Episode Reward: 1.0\n",
      "Step 106 (8999) @ Episode 35/100, loss: 5.1432747568469495e-05\n",
      "Copied model parameters to target network.\n",
      "Step 286 (9179) @ Episode 35/100, loss: 6.801528797950596e-05\n",
      "Episode Reward: 2.0\n",
      "Step 179 (9358) @ Episode 36/100, loss: 0.0002591966185718775\n",
      "Episode Reward: 0.0\n",
      "Step 176 (9534) @ Episode 37/100, loss: 6.467750063166022e-05\n",
      "Episode Reward: 0.0\n",
      "Step 319 (9853) @ Episode 38/100, loss: 4.778772563440725e-05\n",
      "Episode Reward: 3.0\n",
      "Step 146 (9999) @ Episode 39/100, loss: 0.030106892809271812\n",
      "Copied model parameters to target network.\n",
      "Step 439 (10292) @ Episode 39/100, loss: 8.589446952100843e-05\n",
      "Episode Reward: 5.0\n",
      "Step 316 (10608) @ Episode 40/100, loss: 1.6251309716608375e-05\n",
      "Episode Reward: 2.0\n",
      "Step 214 (10822) @ Episode 41/100, loss: 1.9491893908707425e-05\n",
      "Episode Reward: 1.0\n",
      "Step 177 (10999) @ Episode 42/100, loss: 4.9654328904580325e-05\n",
      "Copied model parameters to target network.\n",
      "Step 242 (11064) @ Episode 42/100, loss: 0.03110687807202339\n",
      "Episode Reward: 1.0\n",
      "Step 363 (11427) @ Episode 43/100, loss: 8.161376899806783e-05\n",
      "Episode Reward: 3.0\n",
      "Step 175 (11602) @ Episode 44/100, loss: 9.638216579332948e-05\n",
      "Episode Reward: 0.0\n",
      "Step 313 (11915) @ Episode 45/100, loss: 0.00022873611305840313\n",
      "Episode Reward: 2.0\n",
      "Step 84 (11999) @ Episode 46/100, loss: 0.0001080727597582154\n",
      "Copied model parameters to target network.\n",
      "Step 207 (12122) @ Episode 46/100, loss: 2.8087146347388625e-05\n",
      "Episode Reward: 1.0\n",
      "Step 262 (12384) @ Episode 47/100, loss: 7.391637336695567e-05\n",
      "Episode Reward: 2.0\n",
      "Step 265 (12649) @ Episode 48/100, loss: 7.16129070497118e-05\n",
      "Episode Reward: 2.0\n",
      "Step 240 (12889) @ Episode 49/100, loss: 7.093202293617651e-05\n",
      "Episode Reward: 1.0\n",
      "Step 110 (12999) @ Episode 50/100, loss: 0.0313342846930027\n",
      "Copied model parameters to target network.\n",
      "Step 202 (13091) @ Episode 50/100, loss: 3.3280935895163566e-05\n",
      "Episode Reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-16 02:19:40,425] Starting new video recorder writing to /Users/leena/Documents/AI/reinforcement-learning/DQN/experiments-100/Breakout-v0/monitor/openaigym.video.0.4011.video000050.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 184 (13275) @ Episode 51/100, loss: 1.696057734079659e-05\n",
      "Episode Reward: 0.0\n",
      "Step 171 (13446) @ Episode 52/100, loss: 3.1886818760540336e-05\n",
      "Episode Reward: 0.0\n",
      "Step 168 (13614) @ Episode 53/100, loss: 5.614896508632228e-05\n",
      "Episode Reward: 0.0\n",
      "Step 159 (13773) @ Episode 54/100, loss: 0.00012172339484095573\n",
      "Episode Reward: 0.0\n",
      "Step 226 (13999) @ Episode 55/100, loss: 0.00016653320926707238\n",
      "Copied model parameters to target network.\n",
      "Step 245 (14018) @ Episode 55/100, loss: 6.69665023451671e-05\n",
      "Episode Reward: 1.0\n",
      "Step 171 (14189) @ Episode 56/100, loss: 0.030586162582039833\n",
      "Episode Reward: 0.0\n",
      "Step 177 (14366) @ Episode 57/100, loss: 0.03106698766350746\n",
      "Episode Reward: 0.0\n",
      "Step 203 (14569) @ Episode 58/100, loss: 5.3500749345403165e-05\n",
      "Episode Reward: 1.0\n",
      "Step 287 (14856) @ Episode 59/100, loss: 0.031656090170145035"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# directory to save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments-100/{}\".format(env.spec.id))\n",
    "\n",
    "# creating a global step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# creating both the estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# initializing state processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=100,\n",
    "                                    replay_memory_size=50000,\n",
    "                                    replay_memory_init_size=5000,\n",
    "                                    update_target_estimator_every=1000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.75,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))\n",
    "        np.save('Episode_Reward_%s' %t, stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
